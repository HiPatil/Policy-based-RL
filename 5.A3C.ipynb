{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Shape: (17,) \n",
      "Action Shape: Box(6,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v2')\n",
    "print('Observation Shape:', env.observation_space.shape, '\\nAction Shape:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DISCOUNT = 0.99\n",
    "EPS = 1\n",
    "EPS_DECAY = 0.9999\n",
    "END_EPS = 0.1\n",
    "\n",
    "N_EPISODE = 3000\n",
    "\n",
    "UPDATE_GLOBAL = 100\n",
    "\n",
    "\n",
    "# Dimensions of input and output of environment\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, observations, actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.a = nn.Linear(observations, 128)\n",
    "        self.mean = nn.Linear(128, actions)\n",
    "        self.variance = nn.Linear(128, actions)\n",
    "        self.acti_tanh = nn.Tanh()\n",
    "\n",
    "        self.c = nn.Linear(observations, 128)\n",
    "        self.value = nn.Linear(128, actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.relu6(self.a(x))\n",
    "        mean = 2*self.acti_tanh(self.mean(a))\n",
    "        variance = F.softplus(self.variance(a)) + 0.0001    # to avoid zero value\n",
    "\n",
    "        c = F.relu6(self.c(x))\n",
    "        value = self.value(c)\n",
    "\n",
    "        return mean, variance, value\n",
    "    \n",
    "    def actor_action(self, state):\n",
    "        mean, variance, _ = self.forward(state)\n",
    "        \n",
    "        m = torch.distributions.Normal(mean, torch.sqrt(variance))\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        entropy = 0.5 + 0.5*math.log(2*math.pi) + torch.log(m.scale)\n",
    "        return action.detach().cpu().numpy(), log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(local_net, next_state, rewards, done, discount = DISCOUNT):\n",
    "#     q_val = critic(state)\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _,_, next_q_val = local_net(next_state)\n",
    "    returns = []\n",
    "#     print(next_q_val, rewards)\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        next_q_val = rewards[step] + discount*next_q_val*(1-done[step])\n",
    "        returns.append(next_q_val)\n",
    "        \n",
    "    returns.reverse()\n",
    "    \n",
    "    return returns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(log_probs, q_vals, values, entropies, local_net):\n",
    "\n",
    "#     print('Entropy:',entropies)\n",
    "#     print('log_probs', log_probs)\n",
    "    advantage = q_vals - values\n",
    "\n",
    "    critic_loss = advantage.pow(2)\n",
    "\n",
    "    actor_loss = -(log_probs*advantage.detach())\n",
    "    \n",
    "    total_loss = (actor_loss+critic_loss).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    for lp, gp in zip(local_net.parameters(), global_net.parameters()):\n",
    "        gp._grad = lp.grad\n",
    "        \n",
    "    optimizer.step()\n",
    "    \n",
    "    local_net.load_state_dict(global_net.state_dict())\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(global_ep, global_ep_r, ep_r, res_queue, name):\n",
    "    with global_ep.get_lock():\n",
    "        global_ep.value += 1\n",
    "    with global_ep_r.get_lock():\n",
    "        if global_ep_r.value == 0.:\n",
    "            global_ep_r.value = ep_r\n",
    "        else:\n",
    "            global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01\n",
    "    res_queue.put(global_ep_r.value)\n",
    "    print(\n",
    "        name,\n",
    "        \"Ep:\", global_ep.value,\n",
    "        \"| Ep_r: %.0f\" % global_ep_r.value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trainer(global_net, optimizer, name):\n",
    "    name = 'trainer_' + str(name)\n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    local_net = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    \n",
    "    total_steps = 0\n",
    "    for i in range(1, N_EPISODE+1):\n",
    "        ep_rewards = []\n",
    "        log_probs = []\n",
    "        done_states = []\n",
    "        values = []\n",
    "        entropies = []\n",
    "        total_ep_reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action, log_prob, entropy = local_net.actor_action(state)\n",
    "            \n",
    "            _,_, value = local_net(state)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            ep_rewards.append(torch.tensor(reward, dtype = torch.float, device = device))\n",
    "            done_states.append(torch.tensor(done, dtype = torch.float, device = device))\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            entropies.append(entropy)\n",
    "            total_ep_reward += reward\n",
    "            \n",
    "#             if total_steps%UPDATE_GLOBAL == 0 or done:\n",
    "                \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "        \n",
    "#         queue.put(total_ep_reward)\n",
    "        q_vals = compute_returns(local_net, next_state, ep_rewards, done_states)\n",
    "                \n",
    "        q_vals = torch.cat(q_vals)\n",
    "        values = torch.cat(values)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        entropies = torch.cat(entropies)\n",
    "                \n",
    "        loss = update(log_probs, q_vals, values, entropies, local_net)\n",
    "        if i%10 == 0:\n",
    "            print(name,\"Ep:\", i,\"| Ep_r: %.2f\" % total_ep_reward, \"|  Loss: %.2f\" %loss)\n",
    "#     queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_2 Ep: 10 | Ep_r: -2071.77 |  Loss: 32542.86\n",
      "trainer_3 Ep: 10 | Ep_r: -1999.54 |  Loss: 30562.99\n",
      "trainer_7 Ep: 10 | Ep_r: -2107.89 |  Loss: 33215.70\n",
      "trainer_6 Ep: 10 | Ep_r: -2054.51 |  Loss: 31554.07\n",
      "trainer_4 Ep: 10 | Ep_r: -2102.92 |  Loss: 33656.02\n",
      "trainer_5 Ep: 10 | Ep_r: -2252.59 |  Loss: 37880.36\n",
      "trainer_1 Ep: 10 | Ep_r: -2145.64 |  Loss: 34636.93\n",
      "trainer_0 Ep: 10 | Ep_r: -2230.55 |  Loss: 36800.64\n",
      "trainer_2 Ep: 20 | Ep_r: -2769.58 |  Loss: 49351.78\n",
      "trainer_1 Ep: 20 | Ep_r: -2778.99 |  Loss: 49148.40\n",
      "trainer_5 Ep: 20 | Ep_r: -2792.61 |  Loss: 49051.68\n",
      "trainer_3 Ep: 20 | Ep_r: -2813.89 |  Loss: 49846.34\n",
      "trainer_4 Ep: 20 | Ep_r: -2997.36 |  Loss: 56841.24\n",
      "trainer_7 Ep: 20 | Ep_r: -2931.78 |  Loss: 53985.32\n",
      "trainer_6 Ep: 20 | Ep_r: -2858.35 |  Loss: 50350.19\n",
      "trainer_0 Ep: 20 | Ep_r: -2969.24 |  Loss: 55903.90\n",
      "trainer_2 Ep: 30 | Ep_r: -2963.77 |  Loss: 42695.28\n",
      "trainer_3 Ep: 30 | Ep_r: -2998.93 |  Loss: 43786.80\n",
      "trainer_4 Ep: 30 | Ep_r: -2949.65 |  Loss: 40324.19\n",
      "trainer_1 Ep: 30 | Ep_r: -2924.78 |  Loss: 39125.62\n",
      "trainer_5 Ep: 30 | Ep_r: -2880.08 |  Loss: 37861.13\n",
      "trainer_6 Ep: 30 | Ep_r: -2965.82 |  Loss: 40801.81\n",
      "trainer_7 Ep: 30 | Ep_r: -2938.79 |  Loss: 39336.53\n",
      "trainer_0 Ep: 30 | Ep_r: -3266.26 |  Loss: 50783.62\n",
      "trainer_2 Ep: 40 | Ep_r: -2882.64 |  Loss: 27991.80\n",
      "trainer_3 Ep: 40 | Ep_r: -2863.80 |  Loss: 27073.88\n",
      "trainer_5 Ep: 40 | Ep_r: -2805.68 |  Loss: 24490.46\n",
      "trainer_1 Ep: 40 | Ep_r: -2965.11 |  Loss: 28495.08\n",
      "trainer_7 Ep: 40 | Ep_r: -2855.33 |  Loss: 25242.17\n",
      "trainer_4 Ep: 40 | Ep_r: -2919.27 |  Loss: 27492.45\n",
      "trainer_6 Ep: 40 | Ep_r: -2905.34 |  Loss: 25812.04\n",
      "trainer_0 Ep: 40 | Ep_r: -2899.04 |  Loss: 25273.91\n",
      "trainer_3 Ep: 50 | Ep_r: -2879.02 |  Loss: 17012.80\n",
      "trainer_2 Ep: 50 | Ep_r: -2865.04 |  Loss: 16405.41\n",
      "trainer_7 Ep: 50 | Ep_r: -2947.26 |  Loss: 18696.01\n",
      "trainer_4 Ep: 50 | Ep_r: -2980.75 |  Loss: 17296.12\n",
      "trainer_1 Ep: 50 | Ep_r: -2850.05 |  Loss: 15421.34\n",
      "trainer_5 Ep: 50 | Ep_r: -2894.16 |  Loss: 16210.71\n",
      "trainer_6 Ep: 50 | Ep_r: -2889.71 |  Loss: 16757.23\n",
      "trainer_0 Ep: 50 | Ep_r: -2977.99 |  Loss: 17780.26\n",
      "trainer_3 Ep: 60 | Ep_r: -3092.62 |  Loss: 15713.00\n",
      "trainer_2 Ep: 60 | Ep_r: -3088.84 |  Loss: 15632.55\n",
      "trainer_7 Ep: 60 | Ep_r: -3031.19 |  Loss: 14301.99\n",
      "trainer_1 Ep: 60 | Ep_r: -3116.76 |  Loss: 16492.12\n",
      "trainer_4 Ep: 60 | Ep_r: -3110.87 |  Loss: 16184.65\n",
      "trainer_6 Ep: 60 | Ep_r: -3110.17 |  Loss: 15576.95\n",
      "trainer_5 Ep: 60 | Ep_r: -3095.99 |  Loss: 15091.99\n",
      "trainer_0 Ep: 60 | Ep_r: -3214.43 |  Loss: 16503.13\n",
      "trainer_3 Ep: 70 | Ep_r: -3165.79 |  Loss: 12458.64\n",
      "trainer_2 Ep: 70 | Ep_r: -3163.51 |  Loss: 12304.93\n",
      "trainer_7 Ep: 70 | Ep_r: -3195.78 |  Loss: 12229.80\n",
      "trainer_4 Ep: 70 | Ep_r: -3128.17 |  Loss: 11550.40\n",
      "trainer_1 Ep: 70 | Ep_r: -3183.04 |  Loss: 11888.81\n",
      "trainer_6 Ep: 70 | Ep_r: -3144.80 |  Loss: 10675.50\n",
      "trainer_5 Ep: 70 | Ep_r: -3199.72 |  Loss: 11914.09\n",
      "trainer_0 Ep: 70 | Ep_r: -3179.70 |  Loss: 10938.94\n",
      "trainer_2 Ep: 80 | Ep_r: -3214.94 |  Loss: 9147.43\n",
      "trainer_3 Ep: 80 | Ep_r: -3175.16 |  Loss: 8230.64\n",
      "trainer_7 Ep: 80 | Ep_r: -3186.82 |  Loss: 8272.93\n",
      "trainer_4 Ep: 80 | Ep_r: -3200.58 |  Loss: 8200.49\n",
      "trainer_1 Ep: 80 | Ep_r: -3206.86 |  Loss: 8227.18\n",
      "trainer_6 Ep: 80 | Ep_r: -3173.21 |  Loss: 8015.17\n",
      "trainer_0 Ep: 80 | Ep_r: -3165.15 |  Loss: 7044.05\n",
      "trainer_5 Ep: 80 | Ep_r: -3164.41 |  Loss: 7054.14\n",
      "trainer_2 Ep: 90 | Ep_r: -3232.93 |  Loss: 6126.28\n",
      "trainer_3 Ep: 90 | Ep_r: -3291.35 |  Loss: 6538.74\n",
      "trainer_4 Ep: 90 | Ep_r: -3287.39 |  Loss: 6781.55\n",
      "trainer_7 Ep: 90 | Ep_r: -3272.11 |  Loss: 6185.01\n",
      "trainer_6 Ep: 90 | Ep_r: -3284.63 |  Loss: 6391.58\n",
      "trainer_1 Ep: 90 | Ep_r: -3263.44 |  Loss: 5928.45\n",
      "trainer_5 Ep: 90 | Ep_r: -3245.41 |  Loss: 5652.53\n",
      "trainer_0 Ep: 90 | Ep_r: -3214.58 |  Loss: 5452.49\n",
      "trainer_2 Ep: 100 | Ep_r: -3130.25 |  Loss: 4221.92\n",
      "trainer_3 Ep: 100 | Ep_r: -3116.33 |  Loss: 4172.66\n",
      "trainer_4 Ep: 100 | Ep_r: -3184.57 |  Loss: 4976.39\n",
      "trainer_7 Ep: 100 | Ep_r: -3088.62 |  Loss: 4112.02\n",
      "trainer_6 Ep: 100 | Ep_r: -3040.03 |  Loss: 3936.15\n",
      "trainer_1 Ep: 100 | Ep_r: -3131.10 |  Loss: 4247.56\n",
      "trainer_0 Ep: 100 | Ep_r: -3095.98 |  Loss: 4004.62\n",
      "trainer_5 Ep: 100 | Ep_r: -3114.66 |  Loss: 4307.87\n",
      "trainer_2 Ep: 110 | Ep_r: -3031.36 |  Loss: 3723.62\n",
      "trainer_4 Ep: 110 | Ep_r: -3089.92 |  Loss: 3965.88\n",
      "trainer_7 Ep: 110 | Ep_r: -3052.65 |  Loss: 3977.69\n",
      "trainer_3 Ep: 110 | Ep_r: -3106.41 |  Loss: 3839.98\n",
      "trainer_1 Ep: 110 | Ep_r: -3055.91 |  Loss: 4305.07\n",
      "trainer_6 Ep: 110 | Ep_r: -3015.91 |  Loss: 3807.85\n",
      "trainer_0 Ep: 110 | Ep_r: -3060.03 |  Loss: 4162.78\n",
      "trainer_5 Ep: 110 | Ep_r: -3125.61 |  Loss: 4353.40\n",
      "trainer_2 Ep: 120 | Ep_r: -2907.65 |  Loss: 4541.92\n",
      "trainer_4 Ep: 120 | Ep_r: -2955.53 |  Loss: 5115.20\n",
      "trainer_7 Ep: 120 | Ep_r: -2950.96 |  Loss: 5017.49\n",
      "trainer_1 Ep: 120 | Ep_r: -2950.96 |  Loss: 4598.43\n",
      "trainer_3 Ep: 120 | Ep_r: -2930.15 |  Loss: 4760.37\n",
      "trainer_0 Ep: 120 | Ep_r: -2934.58 |  Loss: 5035.16\n",
      "trainer_6 Ep: 120 | Ep_r: -2912.09 |  Loss: 4726.58\n",
      "trainer_5 Ep: 120 | Ep_r: -2912.31 |  Loss: 4459.59\n",
      "trainer_2 Ep: 130 | Ep_r: -2980.46 |  Loss: 4116.52\n",
      "trainer_0 Ep: 130 | Ep_r: -2958.29 |  Loss: 3609.87\n",
      "trainer_7 Ep: 130 | Ep_r: -2963.64 |  Loss: 4327.29\n",
      "trainer_4 Ep: 130 | Ep_r: -2985.01 |  Loss: 4312.67\n",
      "trainer_1 Ep: 130 | Ep_r: -3006.81 |  Loss: 3999.68\n",
      "trainer_6 Ep: 130 | Ep_r: -2964.73 |  Loss: 4274.62\n",
      "trainer_3 Ep: 130 | Ep_r: -2949.11 |  Loss: 4075.86\n",
      "trainer_5 Ep: 130 | Ep_r: -2941.40 |  Loss: 4102.34\n",
      "trainer_2 Ep: 140 | Ep_r: -2905.86 |  Loss: 3640.70\n",
      "trainer_1 Ep: 140 | Ep_r: -2888.76 |  Loss: 3879.18\n",
      "trainer_0 Ep: 140 | Ep_r: -2949.72 |  Loss: 3171.82\n",
      "trainer_7 Ep: 140 | Ep_r: -2898.23 |  Loss: 3715.64\n",
      "trainer_6 Ep: 140 | Ep_r: -2883.00 |  Loss: 3985.03\n",
      "trainer_4 Ep: 140 | Ep_r: -2913.66 |  Loss: 3777.53\n",
      "trainer_3 Ep: 140 | Ep_r: -2884.84 |  Loss: 3833.19\n",
      "trainer_5 Ep: 140 | Ep_r: -2883.89 |  Loss: 3907.25\n",
      "trainer_2 Ep: 150 | Ep_r: -2924.45 |  Loss: 3929.68\n",
      "trainer_1 Ep: 150 | Ep_r: -2934.82 |  Loss: 3517.46\n",
      "trainer_7 Ep: 150 | Ep_r: -2901.88 |  Loss: 3718.29\n",
      "trainer_0 Ep: 150 | Ep_r: -2873.07 |  Loss: 3673.47\n",
      "trainer_6 Ep: 150 | Ep_r: -2946.96 |  Loss: 3928.83\n",
      "trainer_4 Ep: 150 | Ep_r: -2876.69 |  Loss: 3553.28\n",
      "trainer_3 Ep: 150 | Ep_r: -2938.06 |  Loss: 3861.82\n",
      "trainer_5 Ep: 150 | Ep_r: -2847.35 |  Loss: 3840.64\n",
      "trainer_2 Ep: 160 | Ep_r: -2839.50 |  Loss: 3926.03\n",
      "trainer_1 Ep: 160 | Ep_r: -2849.67 |  Loss: 3791.14\n",
      "trainer_4 Ep: 160 | Ep_r: -2796.57 |  Loss: 3657.09\n",
      "trainer_7 Ep: 160 | Ep_r: -2859.58 |  Loss: 4066.70\n",
      "trainer_0 Ep: 160 | Ep_r: -2851.53 |  Loss: 3770.49\n",
      "trainer_6 Ep: 160 | Ep_r: -2897.53 |  Loss: 4152.82\n",
      "trainer_3 Ep: 160 | Ep_r: -2859.21 |  Loss: 3519.39\n",
      "trainer_5 Ep: 160 | Ep_r: -2866.81 |  Loss: 3784.33\n",
      "trainer_1 Ep: 170 | Ep_r: -2691.81 |  Loss: 3036.29\n",
      "trainer_2 Ep: 170 | Ep_r: -2742.56 |  Loss: 3271.65\n",
      "trainer_0 Ep: 170 | Ep_r: -2698.94 |  Loss: 3162.30\n",
      "trainer_6 Ep: 170 | Ep_r: -2721.99 |  Loss: 3428.74\n",
      "trainer_4 Ep: 170 | Ep_r: -2702.54 |  Loss: 3037.04\n",
      "trainer_7 Ep: 170 | Ep_r: -2681.67 |  Loss: 3001.49\n",
      "trainer_3 Ep: 170 | Ep_r: -2700.71 |  Loss: 3128.91\n",
      "trainer_5 Ep: 170 | Ep_r: -2617.92 |  Loss: 3163.45\n",
      "trainer_2 Ep: 180 | Ep_r: -2424.00 |  Loss: 3130.52\n",
      "trainer_1 Ep: 180 | Ep_r: -2420.79 |  Loss: 3022.05\n",
      "trainer_0 Ep: 180 | Ep_r: -2379.86 |  Loss: 3258.17\n",
      "trainer_6 Ep: 180 | Ep_r: -2415.31 |  Loss: 3134.65\n",
      "trainer_7 Ep: 180 | Ep_r: -2443.45 |  Loss: 3013.85\n",
      "trainer_3 Ep: 180 | Ep_r: -2392.23 |  Loss: 3010.29\n",
      "trainer_4 Ep: 180 | Ep_r: -2412.30 |  Loss: 3105.53\n",
      "trainer_5 Ep: 180 | Ep_r: -2383.80 |  Loss: 2835.11\n",
      "trainer_2 Ep: 190 | Ep_r: -2372.08 |  Loss: 2755.70\n",
      "trainer_7 Ep: 190 | Ep_r: -2391.59 |  Loss: 2831.21\n",
      "trainer_1 Ep: 190 | Ep_r: -2396.90 |  Loss: 2497.14\n",
      "trainer_6 Ep: 190 | Ep_r: -2364.65 |  Loss: 2784.17\n",
      "trainer_0 Ep: 190 | Ep_r: -2343.80 |  Loss: 2683.77\n",
      "trainer_4 Ep: 190 | Ep_r: -2372.36 |  Loss: 2593.59\n",
      "trainer_3 Ep: 190 | Ep_r: -2381.70 |  Loss: 2723.75\n",
      "trainer_5 Ep: 190 | Ep_r: -2366.15 |  Loss: 2799.06\n",
      "trainer_2 Ep: 200 | Ep_r: -2283.21 |  Loss: 2771.30\n",
      "trainer_1 Ep: 200 | Ep_r: -2259.35 |  Loss: 2707.70\n",
      "trainer_7 Ep: 200 | Ep_r: -2241.12 |  Loss: 2650.92\n",
      "trainer_6 Ep: 200 | Ep_r: -2231.14 |  Loss: 2419.02\n",
      "trainer_3 Ep: 200 | Ep_r: -2261.86 |  Loss: 2580.27\n",
      "trainer_4 Ep: 200 | Ep_r: -2270.33 |  Loss: 2592.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_0 Ep: 200 | Ep_r: -2260.31 |  Loss: 2571.10\n",
      "trainer_5 Ep: 200 | Ep_r: -2242.54 |  Loss: 2597.28\n",
      "trainer_7 Ep: 210 | Ep_r: -2243.07 |  Loss: 2123.15\n",
      "trainer_2 Ep: 210 | Ep_r: -2264.74 |  Loss: 2054.47\n",
      "trainer_6 Ep: 210 | Ep_r: -2238.38 |  Loss: 1973.29\n",
      "trainer_1 Ep: 210 | Ep_r: -2228.27 |  Loss: 1977.40\n",
      "trainer_3 Ep: 210 | Ep_r: -2251.82 |  Loss: 1999.02\n",
      "trainer_4 Ep: 210 | Ep_r: -2268.15 |  Loss: 2034.87\n",
      "trainer_0 Ep: 210 | Ep_r: -2237.38 |  Loss: 2030.99\n",
      "trainer_5 Ep: 210 | Ep_r: -2254.79 |  Loss: 2018.12\n",
      "trainer_7 Ep: 220 | Ep_r: -2227.60 |  Loss: 2273.39\n",
      "trainer_2 Ep: 220 | Ep_r: -2216.73 |  Loss: 2256.51\n",
      "trainer_6 Ep: 220 | Ep_r: -2216.03 |  Loss: 2321.46\n",
      "trainer_4 Ep: 220 | Ep_r: -2198.67 |  Loss: 2243.32\n",
      "trainer_3 Ep: 220 | Ep_r: -2220.79 |  Loss: 2226.12\n",
      "trainer_0 Ep: 220 | Ep_r: -2214.68 |  Loss: 2280.45\n",
      "trainer_1 Ep: 220 | Ep_r: -2218.14 |  Loss: 2189.52\n",
      "trainer_5 Ep: 220 | Ep_r: -2209.16 |  Loss: 2260.36\n",
      "trainer_7 Ep: 230 | Ep_r: -2169.23 |  Loss: 2146.54\n",
      "trainer_2 Ep: 230 | Ep_r: -2176.21 |  Loss: 2156.01\n",
      "trainer_6 Ep: 230 | Ep_r: -2176.46 |  Loss: 2112.26\n",
      "trainer_4 Ep: 230 | Ep_r: -2176.47 |  Loss: 2112.60\n",
      "trainer_0 Ep: 230 | Ep_r: -2157.85 |  Loss: 2191.34\n",
      "trainer_3 Ep: 230 | Ep_r: -2153.78 |  Loss: 2211.54\n",
      "trainer_1 Ep: 230 | Ep_r: -2156.23 |  Loss: 2135.04\n",
      "trainer_5 Ep: 230 | Ep_r: -2160.09 |  Loss: 2163.50\n",
      "trainer_7 Ep: 240 | Ep_r: -2150.62 |  Loss: 1934.24\n",
      "trainer_6 Ep: 240 | Ep_r: -2128.30 |  Loss: 1825.78\n",
      "trainer_2 Ep: 240 | Ep_r: -2131.00 |  Loss: 1903.78\n",
      "trainer_4 Ep: 240 | Ep_r: -2121.33 |  Loss: 1900.83\n",
      "trainer_1 Ep: 240 | Ep_r: -2132.47 |  Loss: 1871.28\n",
      "trainer_0 Ep: 240 | Ep_r: -2152.70 |  Loss: 1893.18\n",
      "trainer_3 Ep: 240 | Ep_r: -2114.57 |  Loss: 1847.75\n",
      "trainer_5 Ep: 240 | Ep_r: -2144.07 |  Loss: 1910.77\n",
      "trainer_7 Ep: 250 | Ep_r: -2143.96 |  Loss: 2078.10\n",
      "trainer_6 Ep: 250 | Ep_r: -2124.15 |  Loss: 2073.70\n",
      "trainer_2 Ep: 250 | Ep_r: -2128.11 |  Loss: 2090.89\n",
      "trainer_4 Ep: 250 | Ep_r: -2130.49 |  Loss: 2106.00\n",
      "trainer_0 Ep: 250 | Ep_r: -2126.51 |  Loss: 2130.38\n",
      "trainer_3 Ep: 250 | Ep_r: -2084.03 |  Loss: 2169.08\n",
      "trainer_1 Ep: 250 | Ep_r: -2106.82 |  Loss: 2049.42\n",
      "trainer_5 Ep: 250 | Ep_r: -2110.89 |  Loss: 2079.97\n",
      "trainer_7 Ep: 260 | Ep_r: -2070.23 |  Loss: 2080.17\n",
      "trainer_6 Ep: 260 | Ep_r: -2054.69 |  Loss: 1944.99\n",
      "trainer_2 Ep: 260 | Ep_r: -2033.33 |  Loss: 1968.98\n",
      "trainer_0 Ep: 260 | Ep_r: -2050.59 |  Loss: 1936.88\n",
      "trainer_4 Ep: 260 | Ep_r: -2043.48 |  Loss: 1908.37\n",
      "trainer_3 Ep: 260 | Ep_r: -2039.97 |  Loss: 1845.16\n",
      "trainer_1 Ep: 260 | Ep_r: -2020.29 |  Loss: 1953.55\n",
      "trainer_5 Ep: 260 | Ep_r: -2031.89 |  Loss: 1905.27\n",
      "trainer_7 Ep: 270 | Ep_r: -2001.61 |  Loss: 1708.15\n",
      "trainer_6 Ep: 270 | Ep_r: -2006.06 |  Loss: 1715.47\n",
      "trainer_2 Ep: 270 | Ep_r: -2004.74 |  Loss: 1695.64\n",
      "trainer_0 Ep: 270 | Ep_r: -2013.36 |  Loss: 1712.85\n",
      "trainer_4 Ep: 270 | Ep_r: -2000.78 |  Loss: 1682.49\n",
      "trainer_3 Ep: 270 | Ep_r: -2006.47 |  Loss: 1690.63\n",
      "trainer_1 Ep: 270 | Ep_r: -2001.86 |  Loss: 1662.32\n",
      "trainer_5 Ep: 270 | Ep_r: -2018.29 |  Loss: 1733.92\n",
      "trainer_6 Ep: 280 | Ep_r: -1959.50 |  Loss: 1665.04\n",
      "trainer_2 Ep: 280 | Ep_r: -1962.84 |  Loss: 1571.02\n",
      "trainer_7 Ep: 280 | Ep_r: -1959.88 |  Loss: 1545.60\n",
      "trainer_3 Ep: 280 | Ep_r: -1965.16 |  Loss: 1586.93\n",
      "trainer_1 Ep: 280 | Ep_r: -1962.06 |  Loss: 1581.39\n",
      "trainer_0 Ep: 280 | Ep_r: -1960.64 |  Loss: 1632.94\n",
      "trainer_4 Ep: 280 | Ep_r: -1967.68 |  Loss: 1636.96\n",
      "trainer_5 Ep: 280 | Ep_r: -1958.00 |  Loss: 1636.75\n",
      "trainer_6 Ep: 290 | Ep_r: -1996.26 |  Loss: 1651.04\n",
      "trainer_7 Ep: 290 | Ep_r: -2008.91 |  Loss: 1750.77\n",
      "trainer_1 Ep: 290 | Ep_r: -2010.48 |  Loss: 1695.73\n",
      "trainer_3 Ep: 290 | Ep_r: -1990.86 |  Loss: 1681.42\n",
      "trainer_2 Ep: 290 | Ep_r: -2007.03 |  Loss: 1713.80\n",
      "trainer_0 Ep: 290 | Ep_r: -1994.45 |  Loss: 1692.35\n",
      "trainer_4 Ep: 290 | Ep_r: -1984.33 |  Loss: 1667.40\n",
      "trainer_5 Ep: 290 | Ep_r: -2003.65 |  Loss: 1777.62\n",
      "trainer_6 Ep: 300 | Ep_r: -1972.22 |  Loss: 1713.51\n",
      "trainer_1 Ep: 300 | Ep_r: -1960.72 |  Loss: 1716.91\n",
      "trainer_3 Ep: 300 | Ep_r: -1956.32 |  Loss: 1696.11\n",
      "trainer_7 Ep: 300 | Ep_r: -1963.07 |  Loss: 1756.29\n",
      "trainer_2 Ep: 300 | Ep_r: -1956.44 |  Loss: 1775.59\n",
      "trainer_0 Ep: 300 | Ep_r: -1957.55 |  Loss: 1764.15\n",
      "trainer_4 Ep: 300 | Ep_r: -1956.92 |  Loss: 1658.85\n",
      "trainer_5 Ep: 300 | Ep_r: -1952.13 |  Loss: 1678.69\n",
      "trainer_6 Ep: 310 | Ep_r: -1960.52 |  Loss: 1680.23\n",
      "trainer_1 Ep: 310 | Ep_r: -1949.97 |  Loss: 1726.13\n",
      "trainer_7 Ep: 310 | Ep_r: -1923.42 |  Loss: 1725.41\n",
      "trainer_3 Ep: 310 | Ep_r: -1950.68 |  Loss: 1673.05\n",
      "trainer_2 Ep: 310 | Ep_r: -1932.14 |  Loss: 1710.59\n",
      "trainer_0 Ep: 310 | Ep_r: -1947.11 |  Loss: 1713.15\n",
      "trainer_4 Ep: 310 | Ep_r: -1947.72 |  Loss: 1734.73\n",
      "trainer_5 Ep: 310 | Ep_r: -1949.15 |  Loss: 1639.40\n",
      "trainer_6 Ep: 320 | Ep_r: -1922.38 |  Loss: 1785.23\n",
      "trainer_1 Ep: 320 | Ep_r: -1927.93 |  Loss: 1747.54\n",
      "trainer_7 Ep: 320 | Ep_r: -1923.67 |  Loss: 1745.79\n",
      "trainer_2 Ep: 320 | Ep_r: -1914.35 |  Loss: 1769.68\n",
      "trainer_3 Ep: 320 | Ep_r: -1935.62 |  Loss: 1709.42\n",
      "trainer_0 Ep: 320 | Ep_r: -1916.14 |  Loss: 1714.53\n",
      "trainer_4 Ep: 320 | Ep_r: -1913.66 |  Loss: 1732.15\n",
      "trainer_5 Ep: 320 | Ep_r: -1904.07 |  Loss: 1697.61\n",
      "trainer_6 Ep: 330 | Ep_r: -1934.78 |  Loss: 1564.96\n",
      "trainer_1 Ep: 330 | Ep_r: -1923.24 |  Loss: 1550.08\n",
      "trainer_2 Ep: 330 | Ep_r: -1917.41 |  Loss: 1569.37\n",
      "trainer_7 Ep: 330 | Ep_r: -1916.07 |  Loss: 1586.86\n",
      "trainer_3 Ep: 330 | Ep_r: -1909.70 |  Loss: 1585.29\n",
      "trainer_5 Ep: 330 | Ep_r: -1918.40 |  Loss: 1560.03\n",
      "trainer_4 Ep: 330 | Ep_r: -1913.61 |  Loss: 1602.81\n",
      "trainer_0 Ep: 330 | Ep_r: -1907.85 |  Loss: 1568.43\n",
      "trainer_1 Ep: 340 | Ep_r: -1862.61 |  Loss: 1462.31\n",
      "trainer_6 Ep: 340 | Ep_r: -1875.66 |  Loss: 1453.78\n",
      "trainer_2 Ep: 340 | Ep_r: -1857.92 |  Loss: 1433.16\n",
      "trainer_7 Ep: 340 | Ep_r: -1877.05 |  Loss: 1501.47\n",
      "trainer_3 Ep: 340 | Ep_r: -1860.37 |  Loss: 1417.16\n",
      "trainer_5 Ep: 340 | Ep_r: -1861.04 |  Loss: 1431.23\n",
      "trainer_4 Ep: 340 | Ep_r: -1877.14 |  Loss: 1490.64\n",
      "trainer_0 Ep: 340 | Ep_r: -1871.92 |  Loss: 1464.70\n",
      "trainer_1 Ep: 350 | Ep_r: -1857.96 |  Loss: 1600.35\n",
      "trainer_6 Ep: 350 | Ep_r: -1842.12 |  Loss: 1442.16\n",
      "trainer_2 Ep: 350 | Ep_r: -1862.06 |  Loss: 1450.37\n",
      "trainer_3 Ep: 350 | Ep_r: -1853.44 |  Loss: 1479.19\n",
      "trainer_5 Ep: 350 | Ep_r: -1847.86 |  Loss: 1468.67\n",
      "trainer_7 Ep: 350 | Ep_r: -1872.01 |  Loss: 1494.70\n",
      "trainer_4 Ep: 350 | Ep_r: -1881.24 |  Loss: 1483.02\n",
      "trainer_0 Ep: 350 | Ep_r: -1869.20 |  Loss: 1481.67\n",
      "trainer_1 Ep: 360 | Ep_r: -1935.81 |  Loss: 1639.68\n",
      "trainer_6 Ep: 360 | Ep_r: -1921.39 |  Loss: 1542.71\n",
      "trainer_2 Ep: 360 | Ep_r: -1915.28 |  Loss: 1608.08\n",
      "trainer_5 Ep: 360 | Ep_r: -1940.07 |  Loss: 1643.40\n",
      "trainer_7 Ep: 360 | Ep_r: -1944.82 |  Loss: 1618.64\n",
      "trainer_3 Ep: 360 | Ep_r: -1938.00 |  Loss: 1614.08\n",
      "trainer_0 Ep: 360 | Ep_r: -1948.89 |  Loss: 1630.00\n",
      "trainer_4 Ep: 360 | Ep_r: -1962.96 |  Loss: 1633.53\n",
      "trainer_1 Ep: 370 | Ep_r: -2073.95 |  Loss: 1820.02\n",
      "trainer_6 Ep: 370 | Ep_r: -2089.39 |  Loss: 1947.15\n",
      "trainer_5 Ep: 370 | Ep_r: -2173.74 |  Loss: 2159.14\n",
      "trainer_2 Ep: 370 | Ep_r: -2188.68 |  Loss: 2361.44\n",
      "trainer_7 Ep: 370 | Ep_r: -2203.95 |  Loss: 2220.56\n",
      "trainer_3 Ep: 370 | Ep_r: -2195.10 |  Loss: 2247.02\n",
      "trainer_0 Ep: 370 | Ep_r: -2123.80 |  Loss: 1953.18\n",
      "trainer_4 Ep: 370 | Ep_r: -1950.41 |  Loss: 1610.03\n",
      "trainer_1 Ep: 380 | Ep_r: -1905.80 |  Loss: 1703.66\n",
      "trainer_6 Ep: 380 | Ep_r: -1900.14 |  Loss: 1611.84\n",
      "trainer_5 Ep: 380 | Ep_r: -1892.09 |  Loss: 1678.97\n",
      "trainer_2 Ep: 380 | Ep_r: -1892.61 |  Loss: 1770.15\n",
      "trainer_7 Ep: 380 | Ep_r: -1870.89 |  Loss: 1780.80\n",
      "trainer_4 Ep: 380 | Ep_r: -1966.03 |  Loss: 1935.14\n",
      "trainer_3 Ep: 380 | Ep_r: -1935.21 |  Loss: 1722.46\n",
      "trainer_0 Ep: 380 | Ep_r: -1917.19 |  Loss: 1774.92\n",
      "trainer_1 Ep: 390 | Ep_r: -1908.40 |  Loss: 1831.28\n",
      "trainer_6 Ep: 390 | Ep_r: -1950.89 |  Loss: 1753.00\n",
      "trainer_5 Ep: 390 | Ep_r: -1894.31 |  Loss: 1736.73\n",
      "trainer_2 Ep: 390 | Ep_r: -1910.92 |  Loss: 1940.94\n",
      "trainer_7 Ep: 390 | Ep_r: -1939.03 |  Loss: 1816.54\n",
      "trainer_3 Ep: 390 | Ep_r: -1930.27 |  Loss: 1612.56\n",
      "trainer_4 Ep: 390 | Ep_r: -1874.67 |  Loss: 1662.57\n",
      "trainer_0 Ep: 390 | Ep_r: -1919.48 |  Loss: 1842.58\n",
      "trainer_6 Ep: 400 | Ep_r: -1937.18 |  Loss: 1521.47\n",
      "trainer_1 Ep: 400 | Ep_r: -1914.80 |  Loss: 1706.27\n",
      "trainer_5 Ep: 400 | Ep_r: -1854.70 |  Loss: 1704.97\n",
      "trainer_2 Ep: 400 | Ep_r: -1904.53 |  Loss: 1614.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_7 Ep: 400 | Ep_r: -1968.09 |  Loss: 2022.93\n",
      "trainer_3 Ep: 400 | Ep_r: -1949.35 |  Loss: 1750.27\n",
      "trainer_4 Ep: 400 | Ep_r: -1935.63 |  Loss: 1688.08\n",
      "trainer_0 Ep: 400 | Ep_r: -1938.13 |  Loss: 1598.02\n",
      "trainer_1 Ep: 410 | Ep_r: -1982.24 |  Loss: 1632.11\n",
      "trainer_6 Ep: 410 | Ep_r: -1925.69 |  Loss: 1763.31\n",
      "trainer_4 Ep: 410 | Ep_r: -1939.12 |  Loss: 1532.30\n",
      "trainer_7 Ep: 410 | Ep_r: -1931.80 |  Loss: 1700.32\n",
      "trainer_5 Ep: 410 | Ep_r: -1922.53 |  Loss: 1550.73\n",
      "trainer_2 Ep: 410 | Ep_r: -1919.23 |  Loss: 1632.46\n",
      "trainer_3 Ep: 410 | Ep_r: -1920.56 |  Loss: 1507.30\n",
      "trainer_0 Ep: 410 | Ep_r: -1961.15 |  Loss: 1431.44\n",
      "trainer_1 Ep: 420 | Ep_r: -1870.19 |  Loss: 1649.61\n",
      "trainer_6 Ep: 420 | Ep_r: -1895.02 |  Loss: 1383.85\n",
      "trainer_5 Ep: 420 | Ep_r: -1937.18 |  Loss: 1609.98\n",
      "trainer_7 Ep: 420 | Ep_r: -1935.47 |  Loss: 1587.66\n",
      "trainer_3 Ep: 420 | Ep_r: -1910.00 |  Loss: 1431.23\n",
      "trainer_4 Ep: 420 | Ep_r: -1936.53 |  Loss: 1326.49\n",
      "trainer_2 Ep: 420 | Ep_r: -1956.01 |  Loss: 1585.26\n",
      "trainer_0 Ep: 420 | Ep_r: -1930.90 |  Loss: 1572.44\n",
      "trainer_1 Ep: 430 | Ep_r: -1880.51 |  Loss: 1517.16\n",
      "trainer_6 Ep: 430 | Ep_r: -1893.23 |  Loss: 1645.44\n",
      "trainer_5 Ep: 430 | Ep_r: -1875.37 |  Loss: 1870.65\n",
      "trainer_3 Ep: 430 | Ep_r: -1889.27 |  Loss: 1663.06\n",
      "trainer_7 Ep: 430 | Ep_r: -1873.78 |  Loss: 1719.37\n",
      "trainer_4 Ep: 430 | Ep_r: -1917.10 |  Loss: 1831.89\n",
      "trainer_2 Ep: 430 | Ep_r: -1881.83 |  Loss: 1620.64\n",
      "trainer_0 Ep: 430 | Ep_r: -1889.90 |  Loss: 1584.45\n",
      "trainer_1 Ep: 440 | Ep_r: -1835.77 |  Loss: 1512.74\n",
      "trainer_6 Ep: 440 | Ep_r: -1821.52 |  Loss: 1593.33\n",
      "trainer_5 Ep: 440 | Ep_r: -1800.38 |  Loss: 1639.32\n",
      "trainer_7 Ep: 440 | Ep_r: -1838.66 |  Loss: 1454.23\n",
      "trainer_3 Ep: 440 | Ep_r: -1794.78 |  Loss: 1306.90\n",
      "trainer_4 Ep: 440 | Ep_r: -1784.11 |  Loss: 1579.49\n",
      "trainer_2 Ep: 440 | Ep_r: -1749.39 |  Loss: 1378.79\n",
      "trainer_0 Ep: 440 | Ep_r: -1836.27 |  Loss: 1645.78\n",
      "trainer_6 Ep: 450 | Ep_r: -1773.04 |  Loss: 1401.46\n",
      "trainer_1 Ep: 450 | Ep_r: -1790.46 |  Loss: 1421.03\n",
      "trainer_5 Ep: 450 | Ep_r: -1794.56 |  Loss: 1258.93\n",
      "trainer_7 Ep: 450 | Ep_r: -1794.70 |  Loss: 1540.44\n",
      "trainer_3 Ep: 450 | Ep_r: -1783.15 |  Loss: 1499.24\n",
      "trainer_4 Ep: 450 | Ep_r: -1783.18 |  Loss: 1495.33\n",
      "trainer_2 Ep: 450 | Ep_r: -1785.24 |  Loss: 1196.56\n",
      "trainer_0 Ep: 450 | Ep_r: -1768.98 |  Loss: 1373.68\n",
      "trainer_6 Ep: 460 | Ep_r: -1806.86 |  Loss: 1284.72\n",
      "trainer_1 Ep: 460 | Ep_r: -1786.81 |  Loss: 1300.37\n",
      "trainer_7 Ep: 460 | Ep_r: -1838.81 |  Loss: 1553.17\n",
      "trainer_5 Ep: 460 | Ep_r: -1817.01 |  Loss: 1418.94\n",
      "trainer_3 Ep: 460 | Ep_r: -1884.77 |  Loss: 1694.25\n",
      "trainer_4 Ep: 460 | Ep_r: -1857.15 |  Loss: 1476.42\n",
      "trainer_2 Ep: 460 | Ep_r: -1869.70 |  Loss: 1571.46\n",
      "trainer_0 Ep: 460 | Ep_r: -1842.72 |  Loss: 1471.83\n",
      "trainer_6 Ep: 470 | Ep_r: -1901.85 |  Loss: 1663.94\n",
      "trainer_1 Ep: 470 | Ep_r: -1862.87 |  Loss: 1359.59\n",
      "trainer_7 Ep: 470 | Ep_r: -1891.18 |  Loss: 1428.91\n",
      "trainer_5 Ep: 470 | Ep_r: -1834.19 |  Loss: 1284.63\n",
      "trainer_3 Ep: 470 | Ep_r: -1879.98 |  Loss: 1312.64\n",
      "trainer_4 Ep: 470 | Ep_r: -1908.28 |  Loss: 1520.76\n",
      "trainer_0 Ep: 470 | Ep_r: -2119.04 |  Loss: 9034.35\n",
      "trainer_2 Ep: 470 | Ep_r: -2137.14 |  Loss: 9161.97\n",
      "trainer_6 Ep: 480 | Ep_r: -1888.27 |  Loss: 1499.42\n",
      "trainer_1 Ep: 480 | Ep_r: -1905.02 |  Loss: 1617.07\n",
      "trainer_7 Ep: 480 | Ep_r: -1948.32 |  Loss: 1617.88\n",
      "trainer_3 Ep: 480 | Ep_r: -1887.60 |  Loss: 1586.52\n",
      "trainer_5 Ep: 480 | Ep_r: -1933.41 |  Loss: 1561.97\n",
      "trainer_4 Ep: 480 | Ep_r: -1922.63 |  Loss: 1599.90\n",
      "trainer_2 Ep: 480 | Ep_r: -1948.87 |  Loss: 1717.55\n",
      "trainer_0 Ep: 480 | Ep_r: -1966.26 |  Loss: 1716.29\n",
      "trainer_6 Ep: 490 | Ep_r: -2093.13 |  Loss: 7472.42\n",
      "trainer_1 Ep: 490 | Ep_r: -1923.01 |  Loss: 1555.68\n",
      "trainer_3 Ep: 490 | Ep_r: -1992.28 |  Loss: 1697.55\n",
      "trainer_7 Ep: 490 | Ep_r: -1974.53 |  Loss: 1656.02\n",
      "trainer_4 Ep: 490 | Ep_r: -1950.06 |  Loss: 1594.23\n",
      "trainer_5 Ep: 490 | Ep_r: -2010.46 |  Loss: 1609.83\n",
      "trainer_2 Ep: 490 | Ep_r: -1997.66 |  Loss: 1686.99\n",
      "trainer_0 Ep: 490 | Ep_r: -1983.70 |  Loss: 1684.26\n",
      "trainer_1 Ep: 500 | Ep_r: -1971.34 |  Loss: 1683.32\n",
      "trainer_6 Ep: 500 | Ep_r: -1951.26 |  Loss: 1714.90\n",
      "trainer_5 Ep: 500 | Ep_r: -1929.76 |  Loss: 1564.13\n",
      "trainer_7 Ep: 500 | Ep_r: -1935.25 |  Loss: 1767.04\n",
      "trainer_3 Ep: 500 | Ep_r: -1941.23 |  Loss: 1655.72\n",
      "trainer_4 Ep: 500 | Ep_r: -1959.81 |  Loss: 1600.31\n",
      "trainer_0 Ep: 500 | Ep_r: -1971.58 |  Loss: 1682.86\n",
      "trainer_2 Ep: 500 | Ep_r: -1943.71 |  Loss: 1594.26\n",
      "trainer_6 Ep: 510 | Ep_r: -1943.68 |  Loss: 1582.21\n",
      "trainer_1 Ep: 510 | Ep_r: -1969.16 |  Loss: 1678.70\n",
      "trainer_5 Ep: 510 | Ep_r: -1970.21 |  Loss: 1595.21\n",
      "trainer_3 Ep: 510 | Ep_r: -1925.35 |  Loss: 1534.75\n",
      "trainer_7 Ep: 510 | Ep_r: -1939.68 |  Loss: 1557.75\n",
      "trainer_4 Ep: 510 | Ep_r: -1911.10 |  Loss: 1507.95\n",
      "trainer_2 Ep: 510 | Ep_r: -1914.47 |  Loss: 1554.71\n",
      "trainer_0 Ep: 510 | Ep_r: -1922.34 |  Loss: 1517.46\n",
      "trainer_6 Ep: 520 | Ep_r: -1863.99 |  Loss: 1604.20\n",
      "trainer_1 Ep: 520 | Ep_r: -1885.01 |  Loss: 1550.71\n",
      "trainer_5 Ep: 520 | Ep_r: -1852.32 |  Loss: 1553.65\n",
      "trainer_3 Ep: 520 | Ep_r: -1849.50 |  Loss: 1395.87\n",
      "trainer_7 Ep: 520 | Ep_r: -1837.05 |  Loss: 1510.08\n",
      "trainer_4 Ep: 520 | Ep_r: -1848.33 |  Loss: 1407.49\n",
      "trainer_2 Ep: 520 | Ep_r: -1830.79 |  Loss: 1547.22\n",
      "trainer_0 Ep: 520 | Ep_r: -1837.44 |  Loss: 1400.57\n",
      "trainer_6 Ep: 530 | Ep_r: -1793.95 |  Loss: 1337.75\n",
      "trainer_1 Ep: 530 | Ep_r: -1776.85 |  Loss: 1335.37\n",
      "trainer_3 Ep: 530 | Ep_r: -1795.03 |  Loss: 1366.20\n",
      "trainer_7 Ep: 530 | Ep_r: -1776.27 |  Loss: 1249.55\n",
      "trainer_5 Ep: 530 | Ep_r: -1804.35 |  Loss: 1401.74\n",
      "trainer_4 Ep: 530 | Ep_r: -1788.74 |  Loss: 1419.81\n",
      "trainer_2 Ep: 530 | Ep_r: -1779.27 |  Loss: 1381.44\n",
      "trainer_0 Ep: 530 | Ep_r: -1766.91 |  Loss: 1393.30\n",
      "trainer_6 Ep: 540 | Ep_r: -1769.44 |  Loss: 1339.06\n",
      "trainer_1 Ep: 540 | Ep_r: -1759.37 |  Loss: 1283.53\n",
      "trainer_5 Ep: 540 | Ep_r: -1761.51 |  Loss: 1294.32\n",
      "trainer_7 Ep: 540 | Ep_r: -1739.54 |  Loss: 1344.08\n",
      "trainer_3 Ep: 540 | Ep_r: -1750.38 |  Loss: 1328.82\n",
      "trainer_4 Ep: 540 | Ep_r: -1741.72 |  Loss: 1283.69\n",
      "trainer_2 Ep: 540 | Ep_r: -1718.16 |  Loss: 1255.90\n",
      "trainer_0 Ep: 540 | Ep_r: -1696.57 |  Loss: 1338.61\n",
      "trainer_6 Ep: 550 | Ep_r: -1721.12 |  Loss: 1245.10\n",
      "trainer_1 Ep: 550 | Ep_r: -1743.63 |  Loss: 1206.79\n",
      "trainer_3 Ep: 550 | Ep_r: -1776.44 |  Loss: 1271.23\n",
      "trainer_7 Ep: 550 | Ep_r: -1751.61 |  Loss: 1276.44\n",
      "trainer_5 Ep: 550 | Ep_r: -1767.31 |  Loss: 1232.44\n",
      "trainer_4 Ep: 550 | Ep_r: -1968.96 |  Loss: 7399.69\n",
      "trainer_2 Ep: 550 | Ep_r: -1796.36 |  Loss: 1248.36\n",
      "trainer_0 Ep: 550 | Ep_r: -1781.14 |  Loss: 1231.10\n",
      "trainer_6 Ep: 560 | Ep_r: -1835.20 |  Loss: 1501.81\n",
      "trainer_1 Ep: 560 | Ep_r: -1797.75 |  Loss: 1328.00\n",
      "trainer_3 Ep: 560 | Ep_r: -1842.43 |  Loss: 1502.61\n",
      "trainer_7 Ep: 560 | Ep_r: -1822.45 |  Loss: 1392.78\n",
      "trainer_5 Ep: 560 | Ep_r: -1825.65 |  Loss: 1395.17\n",
      "trainer_4 Ep: 560 | Ep_r: -1848.40 |  Loss: 1459.62\n",
      "trainer_2 Ep: 560 | Ep_r: -1864.00 |  Loss: 1485.93\n",
      "trainer_0 Ep: 560 | Ep_r: -1855.55 |  Loss: 1490.61\n",
      "trainer_6 Ep: 570 | Ep_r: -1878.26 |  Loss: 1398.12\n",
      "trainer_1 Ep: 570 | Ep_r: -1885.99 |  Loss: 1512.94\n",
      "trainer_3 Ep: 570 | Ep_r: -1856.14 |  Loss: 1381.35\n",
      "trainer_7 Ep: 570 | Ep_r: -1880.16 |  Loss: 1405.48\n",
      "trainer_5 Ep: 570 | Ep_r: -1853.79 |  Loss: 1362.57\n",
      "trainer_4 Ep: 570 | Ep_r: -1882.23 |  Loss: 1406.59\n",
      "trainer_2 Ep: 570 | Ep_r: -1849.22 |  Loss: 1402.72\n",
      "trainer_0 Ep: 570 | Ep_r: -1861.27 |  Loss: 1269.35\n",
      "trainer_6 Ep: 580 | Ep_r: -1858.51 |  Loss: 1256.23\n",
      "trainer_1 Ep: 580 | Ep_r: -1879.74 |  Loss: 1423.67\n",
      "trainer_3 Ep: 580 | Ep_r: -1870.24 |  Loss: 1484.96\n",
      "trainer_7 Ep: 580 | Ep_r: -1860.13 |  Loss: 1471.41\n",
      "trainer_5 Ep: 580 | Ep_r: -1857.31 |  Loss: 1318.52\n",
      "trainer_4 Ep: 580 | Ep_r: -1873.27 |  Loss: 1390.28\n",
      "trainer_0 Ep: 580 | Ep_r: -1890.85 |  Loss: 1511.36\n",
      "trainer_2 Ep: 580 | Ep_r: -1860.47 |  Loss: 1284.48\n",
      "trainer_6 Ep: 590 | Ep_r: -1849.77 |  Loss: 1308.95\n",
      "trainer_1 Ep: 590 | Ep_r: -1849.76 |  Loss: 1300.81\n",
      "trainer_3 Ep: 590 | Ep_r: -1838.92 |  Loss: 1173.61\n",
      "trainer_7 Ep: 590 | Ep_r: -1835.23 |  Loss: 1455.11\n",
      "trainer_5 Ep: 590 | Ep_r: -1906.93 |  Loss: 1466.90\n",
      "trainer_4 Ep: 590 | Ep_r: -1821.44 |  Loss: 1424.43\n",
      "trainer_0 Ep: 590 | Ep_r: -1862.75 |  Loss: 1284.64\n",
      "trainer_2 Ep: 590 | Ep_r: -1857.24 |  Loss: 1389.61\n",
      "trainer_6 Ep: 600 | Ep_r: -1794.36 |  Loss: 1386.77\n",
      "trainer_1 Ep: 600 | Ep_r: -1829.04 |  Loss: 1383.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_7 Ep: 600 | Ep_r: -1817.74 |  Loss: 1375.00\n",
      "trainer_3 Ep: 600 | Ep_r: -1825.39 |  Loss: 1308.56\n",
      "trainer_5 Ep: 600 | Ep_r: -1819.37 |  Loss: 1394.16\n",
      "trainer_4 Ep: 600 | Ep_r: -1803.19 |  Loss: 1258.36\n",
      "trainer_0 Ep: 600 | Ep_r: -1846.96 |  Loss: 1318.57\n",
      "trainer_2 Ep: 600 | Ep_r: -1846.70 |  Loss: 1405.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-90e270e9cafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# trainers = [Trainer(global_net, optimizer, queue, i) for i in range(mp.cpu_count())]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# [t.start() for t in trainers]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 18, in Trainer\n",
      "    action, log_prob, entropy = local_net.actor_action(state)\n",
      "  File \"<ipython-input-5-f3c54046eff6>\", line 23, in actor_action\n",
      "    mean, variance, _ = self.forward(state)\n",
      "  File \"<ipython-input-5-f3c54046eff6>\", line 17, in forward\n",
      "    c = F.relu6(self.c(x))\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/nn/functional.py\", line 1372, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "KeyboardInterrupt\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "Process Process-6:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "Process Process-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "Process Process-2:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-f3c8cfd8d4af>\", line 48, in Trainer\n",
      "    loss = update(log_probs, q_vals, values, entropies, local_net)\n",
      "  File \"<ipython-input-7-76ba4285458e>\", line 14, in update\n",
      "    total_loss.backward()\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/himanshu/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "global_net = ActorCritic(obs_dim, action_dim)\n",
    "global_net.share_memory()\n",
    "optimizer = optim.Adam(global_net.parameters(), lr = LEARNING_RATE)\n",
    "# queue = mp.Queue()\n",
    "\n",
    "\n",
    "processes = []\n",
    "\n",
    "for i in range(mp.cpu_count()):\n",
    "    p = mp.Process(target = Trainer, args = (global_net, optimizer, i))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    \n",
    "for p in processes:\n",
    "    p.join()\n",
    "# trainers = [Trainer(global_net, optimizer, queue, i) for i in range(mp.cpu_count())]\n",
    "# [t.start() for t in trainers]\n",
    "\n",
    "\n",
    "# res = []\n",
    "# while True:\n",
    "#     r = queue.get()\n",
    "#     if r is not None:\n",
    "#         res.append(r)\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# [t.join() for t in trainers]\n",
    "# plt.plot(res)\n",
    "# plt.ylabel('Ep_reward')\n",
    "# plt.xlabel('Step')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avg = []\n",
    "def plot(n_rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    mean = np.mean(n_rewards[-20:])\n",
    "#     plt.subplot(131)\n",
    "    mean_avg.append(mean)\n",
    "    plt.title('Reward: %s' % (mean))\n",
    "    plt.plot(n_rewards)\n",
    "    plt.plot(mean_avg)\n",
    "#   plt.subplot(132)\n",
    "#   plt.title('loss')\n",
    "#   plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reward = []\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "#     actor.load_state_dict(torch.load('/home/himanshu/RL/Policy-based-RL/adv-actor.pth'))\n",
    "#     critic.load_state_dict(torch.load('/home/himanshu/RL/Policy-based-RL/adv-critic.pth'))\n",
    "#     print(\"Model Loaded\")\n",
    "#     actor.to(device)\n",
    "#     critic.to(device)\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action, log_prob, _ = global_net.actor_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "        state = next_state\n",
    "    n_reward.append(total_reward)\n",
    "    plot(n_reward)\n",
    "#     print('Duration till which pole is balanced: ', total_reward)\n",
    "\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
